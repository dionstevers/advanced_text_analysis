{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune transformer model for text (sequence) classification\n",
    "\n",
    "This notebook shows a minimal working example of how to **fine-tune a transformer model** for sequence classification.\n",
    "**Sequence classification** refers to the task of assigning a label to a sequence (of tokens). In our case, the sequence is a sentence (sequence of words).\n",
    "\n",
    "The focus in this notebook lies on the **general workflow**:\n",
    "\n",
    "1. Load the labeled text dataset\n",
    "1. Split the dataset into train, dev, and test splits\n",
    "1. Tokenize the texts in each split\n",
    "1. Define the evaluation metrics that quanticy model performance\n",
    "1. Prepare the model for fine-tuning\n",
    "1. Setup a `Trainer` that handles the model fine-tuning\n",
    "1. Use the `Trainer` to fine-tune on the training split examples, using the dev set examples to monitor performace\n",
    "1. Evaluate on the fine-tuned model in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you run this notebook on colab, you'll need to take a number of extra steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if on colab\n",
    "COLAB = True\n",
    "try:\n",
    "    import google.colab\n",
    "except:\n",
    "    COLAB=False\n",
    "\n",
    "if COLAB:\n",
    "    # install required packages\n",
    "    !pip install -q  scikit-learn==1.5.1 datasets==2.21.0 tokenizers==0.19.1 sentencepiece==0.2.0 protobuf==3.20.3 accelerate==0.33.0 transformers==4.44.1 torch~=2.4.0 seqeval==1.2.2\n",
    "\n",
    "if COLAB:\n",
    "    # download custom utils\n",
    "    !mkdir -p utils\n",
    "    !base_url=https://raw.githubusercontent.com/haukelicht/advanced_text_analysis/main/notebooks/utils\n",
    "    !files=(io.py finetuning.py metrics.py)\n",
    "    !for file in \"${files[@]}\"; do curl -o \"utils/$file\" \"$base_url/$file\"; done\n",
    "\n",
    "import os\n",
    "data_path = os.path.join('..', 'data', 'labeled', 'bestvater_sentiment_2023', '')\n",
    "if COLAB:\n",
    "    'https://raw.githubusercontent.com/haukelicht/advanced_text_analysis/data/labeled/bestvater_sentiment_2023/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the required modules, classes, and functions.\n",
    "\n",
    "Note that some function come from the `utils` folder.\n",
    "These are functions I have defined to handle general tasks, like\n",
    "\n",
    "- reading data from a tabular file (e.g., CV);\n",
    "- splitting the data into train, dev, and test split;\n",
    "- tokenization,\n",
    "- etc.\n",
    "\n",
    "These functions should be general enough for many use cases. \n",
    "You can use them in your researhc if you want.\n",
    "But please double check that they do what you want them to do if you want to publish results that depend on my code ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import read_tabular\n",
    "from utils.finetuning import (\n",
    "    get_device, \n",
    "    split_data, \n",
    "    create_sequence_classification_dataset,\n",
    "    preprocess_sequence_classification_dataset\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from utils.metrics import (\n",
    "    parse_sequence_classifier_prediction_output,\n",
    "    compute_sequence_classification_metrics_binary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "device = get_device()\n",
    "print(f'Using device: {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = data_path + 'bestvater_sentiment_2023-motn_responses_sentiment.tsv'\n",
    "df = read_tabular(fp, columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5417"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    0.565442\n",
       "1    0.434558\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = split_data(df, dev_size=0.15, test_size=0.15, seed=SEED, stratify_by='label', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: always do this on the train split (the model can only be expected to predict classes it also sees during training)\n",
    "label2id = {l: i for i, l in enumerate(data_splits['train'].label.unique())}\n",
    "id2label = {i: l for l, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = DatasetDict({s: create_sequence_classification_dataset(df) for s, df in data_splits.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6436e85fe149d4b747bba5a0302d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3793 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ebcb9480c64869b26acae15aa71857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cfe54dcf5c48b7812dee0358987354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "data_splits = data_splits.map(lambda x: preprocess_sequence_classification_dataset(x, tokenizer=tokenizer, label2id=label2id, truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = data_splits.remove_columns(['text', 'label'])\n",
    "data_splits.set_format('torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model for fine-tuning with a `Trainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the `model_init` function that instantiates a pre-trained model with a sequence classification head that can be  fine-tuned.\n",
    "We will pass this function to the trainer instead of the model itself.\n",
    "The reason for this is that it ensures that everytime we call `trainer.train()` below, we start with a fresh model (i.e., no continued fine-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    \"\"\"Function to instantiate a fine-tunable sequence classification model\"\"\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label2id))\n",
    "    if model.config.problem_type is None:\n",
    "        model.config.problem_type = 'single_label_classification'\n",
    "    if isinstance(id2label[0], str):\n",
    "        model.config.id2label = id2label\n",
    "        model.config.label2id = label2id\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a `compute_metrics` function.\n",
    "This function is there for evaluating predicted against observed labels in some held-out data (the dev split during fine-tuning and the test split afterwards).\n",
    "My implementation reports standard metrics for binary classification (precision, recall, F1-score).\n",
    "\n",
    "If you want to adapt it, \n",
    "\n",
    "- keep the first row and work with the observed and predicted labels (`labels` and `predictions`)\n",
    "- return a dictionary that reports evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    labels, predictions = parse_sequence_classifier_prediction_output(p)\n",
    "    return compute_sequence_classification_metrics_binary(y_true=labels, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the **training arguments**.\n",
    "I have added comments to group arguments based on what they are there for.\n",
    "Here some explanation:\n",
    "\n",
    "- *hyperparameters*: they govern how the model learns from the training data\n",
    "    - `optim`: name of optimization algorithm (handles parameter updating)\n",
    "    - `num_train_epochs`: Number of iterations over all training examples\n",
    "    - `per_device_train_batch_size`: Number of examples grouped per updating step\n",
    "- *evaluation*\n",
    "    - `eval_strategy`: when to evaluate (`'epoch'` means after each epoch, i.e., after every completed iteration over all training split examples)\n",
    "- *model saving:*\n",
    "    - `metric_for_best_model`: When we evaluate at the end of each epoch ( see `eval_strategy`), we get one \"checkpoint\" per epoch. `metric_for_best_model` names the metric that is used to determine which of two models checkpoints performed better in the held-out dev split examples. **Important:** The name must be in the dictionary returned by the `compute_metrics` finction (see below)\n",
    "    - `load_best_model_at_end`: Whether or not to load the best model (judged based on `metric_for_best_model`) should be loaded when finetuning ends. `True` (recommended) means that the `trainer` represents the best model instance (judged based on the `metric_for_best_model` metric, e.g. F1, in the dev split examples). \n",
    "    - `save_total_limit` determines how many checkpoints to save at most. Note that each model checkpoint will have several GB. So set this to a low number (e.g., 2) to avoid spamming your computer. **Important:** Setting this to 2 is the minimal required value if you set `load_best_model_at_end=True`\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to folder where model checkpoints and finetuning logs will be saved\n",
    "dest = './../results/example_classifier/'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=dest,\n",
    "    # hyperparameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    optim='adamw_torch',\n",
    "    # use_mps_device=str(device)=='mps', # uncomment this when using older version of `transformers` library\n",
    "    fp16=str(device).startswith('cuda'),\n",
    "    # evaluation on dev set\n",
    "    eval_strategy='epoch',\n",
    "    # model saving\n",
    "    metric_for_best_model='f1', # use 'f1_macro' for multiclass classification\n",
    "    greater_is_better=True,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    # logging\n",
    "    logging_strategy='epoch',\n",
    "    logging_dir=dest+'logs',\n",
    "    # for reproducibility\n",
    "    seed=SEED,\n",
    "    data_seed=SEED,\n",
    "    full_determinism=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a `Trainer` instance that handles the fine-tuning and dev split evaluation.\n",
    "We call this object `trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=data_splits['train'],\n",
    "    eval_dataset=data_splits['dev'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c343c535f1cb486487767451e783ab72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3803, 'grad_norm': 2.3094935417175293, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381e46562f01420d9e223f2d1fc9b883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26127269864082336, 'eval_accuracy': 0.9014778325123153, 'eval_accuracy_balanced': 0.8978071555975239, 'eval_f1': 0.8847262247838616, 'eval_precision': 0.9002932551319648, 'eval_recall': 0.8696883852691218, 'eval_runtime': 20.7806, 'eval_samples_per_second': 39.075, 'eval_steps_per_second': 1.251, 'epoch': 1.0}\n",
      "{'loss': 0.2125, 'grad_norm': 0.3254886567592621, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df86130f710d44b68f6dc1b0d2001105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26307421922683716, 'eval_accuracy': 0.9187192118226601, 'eval_accuracy_balanced': 0.9150203361168201, 'eval_f1': 0.9046242774566474, 'eval_precision': 0.9233038348082596, 'eval_recall': 0.886685552407932, 'eval_runtime': 8.704, 'eval_samples_per_second': 93.291, 'eval_steps_per_second': 2.987, 'epoch': 2.0}\n",
      "{'loss': 0.1355, 'grad_norm': 0.19413040578365326, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0decab2d64424189fe72e52da655a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3295578956604004, 'eval_accuracy': 0.9273399014778325, 'eval_accuracy_balanced': 0.9262437741857839, 'eval_f1': 0.9165487977369166, 'eval_precision': 0.9152542372881356, 'eval_recall': 0.9178470254957507, 'eval_runtime': 8.5952, 'eval_samples_per_second': 94.471, 'eval_steps_per_second': 3.025, 'epoch': 3.0}\n",
      "{'train_runtime': 463.8785, 'train_samples_per_second': 24.53, 'train_steps_per_second': 1.539, 'train_loss': 0.2427623064912, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=714, training_loss=0.2427623064912, metrics={'train_runtime': 463.8785, 'train_samples_per_second': 24.53, 'train_steps_per_second': 1.539, 'total_flos': 377377822814460.0, 'train_loss': 0.2427623064912, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b071d174d5134b61abefbdb6a0dde70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.43572258949279785,\n",
       " 'test_accuracy': 0.9076354679802956,\n",
       " 'test_accuracy_balanced': 0.906197732476686,\n",
       " 'test_f1': 0.8939179632248939,\n",
       " 'test_precision': 0.8926553672316384,\n",
       " 'test_recall': 0.8951841359773371,\n",
       " 'test_runtime': 29.0263,\n",
       " 'test_samples_per_second': 27.975,\n",
       " 'test_steps_per_second': 0.896,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(data_splits['test'], metric_key_prefix='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_text_analysis_gesis_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
